import numpy as np
import operator
import operator

import theano.tensor as T

from pylearn2.utils import sharedX
from pylearn2.train_extensions import TrainExtension
from pylearn2.datasets.transformer_dataset import TransformerDataset

from parameter_prediction.kernelize import spatial_process

def expand_to_matrix_operator(W_expander, v):
    """Expand v to a matrix operator W using the specified expander.

    Use the expanded operator like: T.dot(x, W).
    """
    return T.dot(W_expander, v)

def expand_to_filter_stack(W_expander, v, shape, batch_size, n_channels, axes=['b',0,1,'c']):
    """Expand v to a filter stack for use with convolutional operators.

    Specify axes if needed.  Shuffling will happen correctly."""
    W = expand_to_matrix_operator(W_expander, v)
    W_b01c = W.T.reshape((batch_size, shape[0], shape[1], n_channels))
    return W_b01c.dimshuffle(*[['b',0,1,'c'].index(x) for x in axes])

def expand_to_matrix_operator_numpy(W_expander, v):
    return np.dot(W_expander, v)

def expand_to_filter_stack_numpy(W_expander, v, shape, batch_size, n_channels, axes=['b',0,1,'c']):
    W = expand_to_matrix_operator_numpy(W_expander, v)
    W_b01c = W.T.reshape((batch_size, shape[0], shape[1], n_channels))
    return W_b01c.transpose([['b',0,1,'c'].index(x) for x in axes])

class KernelExpander(object):
    def __init__(self, spatial_process, smoothing, kernel):
        self.spatial_process = spatial_process
        self.smoothing = smoothing
        self.kernel = kernel
        self.W_expander = None

    def clone(self):
        """Clones do not inherit the generated state"""
        return self.__class__(
            spatial_process=self.spatial_process,
            smoothing=self.smoothing,
            kernel=self.kernel)

    def _expand(self, kernel_points):
        kernel_dim = len(kernel_points)

        kernel_points = np.vstack(kernel_points)
        K = self.kernel.apply(kernel_points, kernel_points)
        K += self.smoothing * np.eye(*K.shape)
        K_inv = np.linalg.inv(K)

        flat_space_coordinates = np.vstack(spatial_process.enumerate_space(
            self.spatial_process.full_shape))
        k = self.kernel.apply(kernel_points, flat_space_coordinates)

        W_expander = np.dot(k.T, K_inv)
        W_expander = sharedX(W_expander)

        return W_expander

    def generate(self):
        if self.W_expander is None:
            self.kernel_points = self.spatial_process.generate()
            self.W_expander = self._expand(self.kernel_points)
            self.expanded_dim = len(self.kernel_points)

        return self.W_expander

class SigmoidJacobianKernelExpander(KernelExpander):
    # """Initially the W_expander generated by this class is the correct shape but
    # full of zeros.  It must be initialized by an EmpiricalKernelInitializer
    # TrainExtension, which will fill in the values in W_expander."""

    def __init__(self, *args, **kwargs):
        kwargs['kernel'] = None
        super(SigmoidJacobianKernelExpander, self).__init__(*args, **kwargs)

    def _expand(self, kernel_points):
        n_vis = reduce(operator.mul, self.spatial_process.full_shape)
        n_points = len(kernel_points)
        W_expander = sharedX(np.zeros((n_vis, n_points)))
        return W_expander

    def setup(self, model, dataset, algorithm):
        assert self.W_expander is not None

        # only implemented for 1d processes
        assert np.all(np.asarray(map(len, self.kernel_points)) == 1)
        kernel_points = np.asarray([x for x, in self.kernel_points])
        kernel_points = kernel_points.reshape((1,-1))

        # need to pull lower level models out of the transformer dataset
        assert isinstance(dataset, TransformerDataset)

        dataset_iter = dataset.iterator(mode='sequential',
                num_batches=1)
        transformed_X = dataset_iter.next()

        # derivative of sigmoid
        dX = transformed_X * (1.-transformed_X)

        W = dataset.transformer.get_weights()

        # grad_v h_j(v) = g'(<v, w_j>)w_j
        # grad_v h(v)   = g'(<v, W>) .*. W    .*. = elementwise multiplication
        #
        # < grad_v h(v) , grad_v h(v) > = directional deriv in direction of other derivs
        #
        # Average these over data
        
        ave_grad_vH = 1./dX.shape[0] * np.einsum('ij,kj -> kj', dX, W)
        cov = np.dot(ave_grad_vH.T, ave_grad_vH)

        K = cov[kernel_points.T, kernel_points]
        k = cov[kernel_points.ravel(), :]

        W_expander = np.linalg.solve(K.T + self.smoothing * np.eye(*K.shape), k).T

        # Set values of W_expander _in_place_.
        self.W_expander.set_value(W_expander.astype(self.W_expander.dtype))


class EmpiricalKernelExpander(KernelExpander):
    # """Initially the W_expander generated by this class is the correct shape but
    # full of zeros.  It must be initialized by an EmpiricalKernelInitializer
    # TrainExtension, which will fill in the values in W_expander."""
    
    def __init__(self, *args, **kwargs):
        kwargs['kernel'] = None
        super(EmpiricalKernelExpander, self).__init__(*args, **kwargs)

    def _expand(self, kernel_points):
        n_vis = reduce(operator.mul, self.spatial_process.full_shape)
        n_points = len(kernel_points)
        W_expander = sharedX(np.zeros((n_vis, n_points)))
        return W_expander

    def setup(self, model, dataset, algorithm):
        assert self.W_expander is not None

        # only implemented for 1d processes
        assert np.all(np.asarray(map(len, self.kernel_points)) == 1)
        kernel_points = np.asarray([x for x, in self.kernel_points])
        kernel_points = kernel_points.reshape((1,-1))
        
        dataset_iter = dataset.iterator(mode='sequential',
                num_batches=1)
        X = dataset_iter.next()

        cov = np.cov(X.T)

        K = cov[kernel_points.T, kernel_points]
        k = cov[kernel_points.ravel(), :]

        W_expander = np.linalg.solve(K.T + self.smoothing * np.eye(*K.shape), k).T

        # Set values of W_expander _in_place_.
        self.W_expander.set_value(W_expander.astype(self.W_expander.dtype))

class EmpiricalSquaredCorrelationKernelExpander(KernelExpander):
    # """Initially the W_expander generated by this class is the correct shape but
    # full of zeros.  It must be initialized by an EmpiricalKernelInitializer
    # TrainExtension, which will fill in the values in W_expander."""
    
    def __init__(self, *args, **kwargs):
        kwargs['kernel'] = None
        super(EmpiricalSquaredCorrelationKernelExpander, self).__init__(*args, **kwargs)

    def _expand(self, kernel_points):
        n_vis = reduce(operator.mul, self.spatial_process.full_shape)
        n_points = len(kernel_points)
        W_expander = sharedX(np.zeros((n_vis, n_points)))
        return W_expander

    def setup(self, model, dataset, algorithm):
        assert self.W_expander is not None

        # only implemented for 1d processes
        assert np.all(np.asarray(map(len, self.kernel_points)) == 1)
        kernel_points = np.asarray([x for x, in self.kernel_points])
        kernel_points = kernel_points.reshape((1,-1))
        
        dataset_iter = dataset.iterator(mode='sequential',
                num_batches=1)
        X = dataset_iter.next()
        X = X**2

        cov = np.cov(X.T)

        K = cov[kernel_points.T, kernel_points]
        k = cov[kernel_points.ravel(), :]

        W_expander = np.linalg.solve(K.T + self.smoothing * np.eye(*K.shape), k).T

        # Set values of W_expander _in_place_.
        self.W_expander.set_value(W_expander.astype(self.W_expander.dtype))


class RandomKernelExpander(KernelExpander):
    # Does _not_ need to be initialized by EmpiricalKernelInitializer
    def __init__(self, *args, **kwargs):
        kwargs['kernel'] = None
        super(RandomKernelExpander, self).__init__(*args, **kwargs)

    def _expand(self, kernel_points):
        n_vis = reduce(operator.mul, self.spatial_process.full_shape)
        n_points = len(kernel_points)

        assert np.all(np.asarray(map(len, self.kernel_points)) == 1)
        kernel_points = np.asarray([x for x, in self.kernel_points])
        kernel_points = kernel_points.reshape((1,-1))

        # random positive definite matrix
        cov = np.random.standard_normal((n_vis, n_vis))
        cov = 0.5*(cov + cov.T)
        w,v = np.linalg.eig(cov)
        cov = np.dot(v, np.dot(np.diag(np.abs(w)), v.T))

        K = cov[kernel_points.T, kernel_points]
        k = cov[kernel_points.ravel(), :]

        W_expander = np.linalg.solve(K.T + self.smoothing * np.eye(*K.shape), k).T

        W_expander = sharedX(W_expander)
        return W_expander


class CompletelyRandomKernelExpander(KernelExpander):
    # Does _not_ need to be initialized by EmpiricalKernelInitializer
    def __init__(self, shape, n_points, *args, **kwargs):
        assert "kernel" not in kwargs
        kwargs['kernel'] = None
        assert "spatial_process" not in kwargs
        kwargs['spatial_process'] = None
        
        super(RandomKernelExpander, self).__init__(*args, **kwargs)
        self.shape = shape
        self.n_points = n_points

    def _expand(self):
        n_vis = reduce(operator.mul, self.shape)

        W_expander = 0.1*np.random.standard_normal((n_vis, self.n_points))
        W_expander = sharedX(W_expander)
        
        return W_expander

    def generate(self):
        if self.W_expander is None:
            self.W_expander = self._expand()
            self.expanded_dim = self.n_points

        return self.W_expander



class FourierExpander(object):
    # Does _not_ need to be initialized by EmpiricalKernelInitializer
    def __init__(self, rank, shape, *args, **kwargs):
        self.shape = shape
        self.rank = rank
        self.W_expander = None

    def clone(self):
        """Clones do not inherit the generated state"""
        return self.__class__(
            rank=self.rank,
            shape=self.shape)

    def _expand(self):
        n_vis = reduce(operator.mul, self.shape)

        k = np.random.choice(np.arange(n_vis), size=self.rank, replace=False)
        k = k.reshape((1,-1))
        n = np.arange(n_vis).reshape((-1,1))

        W_expander = np.exp(-1j*2.0*np.pi*n*k / n_vis)

        return sharedX(W_expander)

    def generate(self):
        if self.W_expander is None:
            self.W_expander = self._expand()
            self.expanded_dim = self.rank

        return self.W_expander


class RandomConnectionExpander(object):
    def __init__(self, rank, shape):
        self.shape = shape
        self.rank = rank
        self.W_expander = None

    def clone(self):
        """Clones do not inherit the generated state"""
        return self.__class__(
            rank=self.rank,
            shape=self.shape)

    def _expand(self):
        n_vis = reduce(operator.mul, self.shape)

        k = np.random.choice(np.arange(n_vis), size=self.rank, replace=False)
        k = k.reshape((1,-1))
        k.sort()
        n = np.arange(n_vis).reshape((-1,1))

        W_expander = np.eye(n_vis)
        W_expander = W_expander[n,k]

        return sharedX(W_expander)


    def generate(self):
        if self.W_expander is None:
            self.W_expander = self._expand()
            self.expanded_dim = self.rank

        return self.W_expander



class EmpiricalKernelInitializer(TrainExtension):
    """Initializes any empirical kernel expanders in the first layer of an MLP."""
    def setup(self, model, dataset, algorithm):
        if hasattr(model,'layers'):
            base_layer = model.layers[0]
        else:
            base_layer = model
        for expander_parameterization in base_layer.kernel_expanders.parameterizations:
            for kernel_expander in expander_parameterization.kernel_expanders:
                #assert isinstance(kernel_expander, EmpiricalKernelExpander)
                kernel_expander.setup(model, dataset, algorithm)


class ExpanderParameterizationSequence(object):
    def __init__(self, parameterizations):
        self.parameterizations = parameterizations

    def generate(self, layer_name, rng):
        expander_parameters = [
            p.generate(name=layer_name + "_p{}".format(i), rng=rng)
            for i, p in enumerate(self.parameterizations) ]

        self.W_expanders = []
        self.vs = []
        for pW_expanders, pvs in expander_parameters:
            self.W_expanders.extend(pW_expanders)
            self.vs.extend(pvs)
        
        return self.W_expanders, self.vs

class ExpanderParameterization(object):
    def __init__(self, n_replicas, n_filters_per_replica, irange, kernel_expander):
        self.__dict__.update(locals())
        del self.self
        del self.kernel_expander

        self.kernel_expanders = [
            kernel_expander.clone()
            for _ in xrange(self.n_replicas) ]


    def generate(self, name, rng):
        assert not hasattr(self, "W_expanders")

        # TODO: propagate rng down the stack
        self.W_expanders = [
            ke.generate() for ke in self.kernel_expanders ]

        self.vs = [
            rng.uniform(
                -self.irange, self.irange,
                (ke.expanded_dim, self.n_filters_per_replica))
            for ke in self.kernel_expanders
            ]
        #self.vs = map(np.ones_like, self.vs) # don't use this, but useful for debugging
        self.vs = map(sharedX, self.vs)

        for i, W_expander in enumerate(self.W_expanders):
            W_expander.name = name + "_W-{}".format(i)

        for i, v in enumerate(self.vs):
            v.name = name + "_v-{}".format(i)
        
        return self.W_expanders, self.vs
